<!DOCTYPE html>
<html lang="en">
<head>
 <title>Automated Feature Selection</title>
 <!-- Latest compiled and minified CSS -->
 <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha384-HSMxcRTRxnN+Bdg0JdbxYKrThecOKuH5zCYotlSAcp1+c8xmyTe9GYg1l9a69psu" crossorigin="anonymous">
 <div class="container">
  <h1><a href=" https://lucashomil.github.io/datascience">Welcome to my blog</a></h1>
 </div>
</head>
<body>
 <div class="container">
<div class="row">
 <div class="col-md-8">
  <h3>Automated Feature Selection</h3>
  <label>2019-06-28</label>
  <p>In our projects, we often deal with datasets containing many variables. Some of them may decrease the accuracy of models. Today, I am going to show you some automatic ways of selecting relevant features.
Imagine that you got a dataset with hundreds of variables and you do not know if all of them are relevant in the predictive modelling problem.</p>
<p>Feature engineering is an essential parameter of a successful model as observed below:</p>
<p>The process of identifying or excluding not necessary variables is called feature selection and in most scenarios it is defined through an automated algorithm.</p>
<p>Why feature selection makes our approach more efficient:
    - It helps to avoid overfitting as less redundant data is present after feature selection.
    - It may increase prediction performance, as learning will concentrate only on meaningful data.
    - It reduces execution time and is memory-efficient as there is less data to process.</p>
<h1>Univariate statistics</h1>
<p>Univariate statistics is a simple method which is by looking at each feature individually and running a statistical test to see whether it is related to the target. This method is also known as analysis of variance (ANOVA)</p>
<p>We will use the Boston Housing Data as an example. Then, we create a new dataset that consists of the Boston Housing Data with an additional 25 completely random features.</p>
<div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_boston</span>
<span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># label columns</span>

<span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span>

<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Price&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span>

<span class="n">noise</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">100</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">boston</span><span class="o">.</span><span class="n">data</span><span class="p">),</span> <span class="mi">25</span><span class="p">)))</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Price&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">noise</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> 

<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Price&#39;</span><span class="p">]</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CRIM</th>
      <th>ZN</th>
      <th>INDUS</th>
      <th>CHAS</th>
      <th>NOX</th>
      <th>RM</th>
      <th>AGE</th>
      <th>DIS</th>
      <th>RAD</th>
      <th>TAX</th>
      <th>PTRATIO</th>
      <th>B</th>
      <th>LSTAT</th>
      <th>Price</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.00632</td>
      <td>18.0</td>
      <td>2.31</td>
      <td>0.0</td>
      <td>0.538</td>
      <td>6.575</td>
      <td>65.2</td>
      <td>4.0900</td>
      <td>1.0</td>
      <td>296.0</td>
      <td>15.3</td>
      <td>396.90</td>
      <td>4.98</td>
      <td>24.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.02731</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>6.421</td>
      <td>78.9</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>396.90</td>
      <td>9.14</td>
      <td>21.6</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.02729</td>
      <td>0.0</td>
      <td>7.07</td>
      <td>0.0</td>
      <td>0.469</td>
      <td>7.185</td>
      <td>61.1</td>
      <td>4.9671</td>
      <td>2.0</td>
      <td>242.0</td>
      <td>17.8</td>
      <td>392.83</td>
      <td>4.03</td>
      <td>34.7</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>


<p>Univariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the transform method:</p>
<div class="highlight"><pre><span></span><span class="o">-</span> <span class="nv">SelectKBest</span> <span class="nv">removes</span> <span class="nv">all</span> <span class="nv">but</span> <span class="nv">the</span>  <span class="nv">highest</span> <span class="nv">scoring</span> <span class="nv">features</span>.

<span class="o">-</span> <span class="nv">SelectPercentile</span> <span class="nv">removes</span> <span class="nv">all</span> <span class="nv">but</span> <span class="nv">a</span> <span class="nv">user</span><span class="o">-</span><span class="nv">specified</span> <span class="nv">highest</span> <span class="nv">scoring</span> <span class="nv">percentage</span> <span class="nv">of</span> <span class="nv">features</span>.

<span class="o">-</span> <span class="nv">Using</span> <span class="nv">common</span> <span class="nv">univariate</span> <span class="nv">statistical</span> <span class="nv">tests</span> <span class="k">for</span> <span class="nv">each</span> <span class="nv">feature</span>: <span class="nv">false</span> <span class="nv">positive</span> <span class="nv">rate</span> <span class="nv">SelectFpr</span>, <span class="nv">false</span> <span class="nv">discovery</span> <span class="nv">rate</span> <span class="nv">SelectFdr</span>, <span class="nv">or</span> <span class="nv">family</span> <span class="nv">wise</span> <span class="nv">error</span> <span class="nv">SelectFwe</span>.
<span class="o">-</span> <span class="nv">GenericUnivariateSelect</span> <span class="nv">allows</span> <span class="nv">to</span> <span class="nv">perform</span> <span class="nv">univariate</span> <span class="nv">feature</span> <span class="nv">selection</span> <span class="nv">with</span> <span class="nv">a</span> <span class="nv">configurable</span> <span class="nv">strategy</span>. <span class="nv">This</span> <span class="nv">allows</span> <span class="nv">to</span> <span class="nv">select</span> <span class="nv">the</span> <span class="nv">best</span> <span class="nv">univariate</span> <span class="nv">selection</span> <span class="nv">strategy</span> <span class="nv">with</span> <span class="nv">hyper</span><span class="o">-</span><span class="nv">parameter</span> <span class="nv">search</span> <span class="nv">estimator</span>.
</pre></div>


<p>We have to define a threshold on the p-value of the statistical test to decide how many features to keep. There are several strategies implemented in scikit-learn, a straight-forward one being SelectPercentile, which selects a percentile of the original features.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectPercentile</span>

<span class="c1"># use f_classif (the default) and SelectPercentile to select percentile = 15:</span>

<span class="n">select</span> <span class="o">=</span> <span class="n">SelectPercentile</span><span class="p">(</span><span class="n">percentile</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
<span class="n">select</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># # transform training set:</span>

<span class="n">X_train_selected</span> <span class="o">=</span> <span class="n">select</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X_train_selected</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">select</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">253</span><span class="p">,</span> <span class="mi">38</span><span class="p">)</span>
<span class="p">(</span><span class="mi">253</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>





<span class="nb">array</span><span class="p">([</span> <span class="k">True</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span>  <span class="k">True</span><span class="p">,</span>  <span class="k">True</span><span class="p">,</span>  <span class="k">True</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span>
       <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span>  <span class="k">True</span><span class="p">,</span>  <span class="k">True</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span>
       <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span>
       <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">,</span>
       <span class="k">False</span><span class="p">,</span> <span class="k">False</span><span class="p">])</span>
</pre></div>


<p>These objects take as input a scoring function that returns univariate scores and p-values (or only scores for SelectKBest and SelectPercentile):</p>
<div class="highlight"><pre><span></span><span class="o">-</span> <span class="k">For</span> <span class="nv">regression</span>: <span class="nv">f_regression</span>, <span class="nv">mutual_info_regression</span>
<span class="o">-</span> <span class="k">For</span> <span class="nv">classification</span>: <span class="nv">chi2</span>, <span class="nv">f_classif</span>, <span class="nv">mutual_info_classif</span>
</pre></div>


<p>The methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">f_classif</span><span class="p">,</span> <span class="n">f_regression</span><span class="p">,</span> <span class="n">chi2</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="n">F</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">f_regression</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="p">[</span><span class="o">&lt;</span><span class="n">matplotlib</span><span class="p">.</span><span class="n">lines</span><span class="p">.</span><span class="n">Line2D</span> <span class="k">at</span> <span class="mi">0</span><span class="n">x1bf8bbc7278</span><span class="o">&gt;</span><span class="p">]</span>
</pre></div>


<p><img alt="png" src="images/output_13_1.png"></p>
<p>Going back to the SelectPercentile transformer, we can obtain the features that are selected using the get_support method:</p>
<div class="highlight"><pre><span></span><span class="n">mask</span> <span class="o">=</span> <span class="n">select</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
<span class="c1"># visualize the mask. black is True, white is False</span>
<span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray_r&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="p">[</span> <span class="k">True</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span>  <span class="k">True</span>  <span class="k">True</span>  <span class="k">True</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span>  <span class="k">True</span>
  <span class="k">True</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span>
 <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span> <span class="k">False</span>
 <span class="k">False</span> <span class="k">False</span><span class="p">]</span>





<span class="o">&lt;</span><span class="n">matplotlib</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="n">AxesImage</span> <span class="k">at</span> <span class="mi">0</span><span class="n">x1bf8bc65240</span><span class="o">&gt;</span>
</pre></div>


<p><img alt="png" src="output_15_2.png"></p>
<p>It's important to learn the feature selection only on the training set!</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># transform test data:</span>
<span class="n">X_test_selected</span> <span class="o">=</span> <span class="n">select</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">ln</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="n">ln</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;Score with all features: {ln.score(X_test, y_test)}&quot;</span><span class="p">)</span>

<span class="n">ln</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_selected</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;Score with only selected features: {ln.score(X_test_selected, y_test)}&quot;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">Score</span> <span class="k">with</span> <span class="k">all</span> <span class="n">features</span><span class="p">:</span> <span class="mi">0</span><span class="p">.</span><span class="mi">6562323137037431</span>
<span class="n">Score</span> <span class="k">with</span> <span class="k">only</span> <span class="n">selected</span> <span class="n">features</span><span class="p">:</span> <span class="mi">0</span><span class="p">.</span><span class="mi">6056841906269855</span>
</pre></div>


<h1>Model-based Feature Selection</h1>
<p>Next, we learn how to select features through a model-based feature selection. This method uses machine learning to model the data, studying the usefulness of a feature according to its relative importance to the predictability of the target variable. In order to do that, the model provides some way to rank the features by importance. </p>
<p>Moreover, the obvious example is linear regression, which works by applying a coefficient multiplier to each of the features. Obviously, the higher the coefficient, the more valuable the feature.</p>
<p>To present model-based selection in scikit-learn, you can use "SelectFromModel" in conjunction with different models. Any of these models can be made into a transformer that does feature selection by wrapping it with the SelectFromModel class:</p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectFromModel</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="n">select</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">select</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">X_train_rf</span> <span class="o">=</span> <span class="n">select</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">X_train_rf</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">253</span><span class="p">,</span> <span class="mi">38</span><span class="p">)</span>
<span class="p">(</span><span class="mi">253</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">mask</span> <span class="o">=</span> <span class="n">select</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>
<span class="c1"># visualize the mask. black is True, white is False</span>
<span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray_r&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">matplotlib</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="n">AxesImage</span> <span class="k">at</span> <span class="mi">0</span><span class="n">x1bf8c26b668</span><span class="o">&gt;</span>
</pre></div>


<p><img alt="png" src="output_22_1.png"></p>
<div class="highlight"><pre><span></span><span class="n">X_test_rf</span> <span class="o">=</span> <span class="n">select</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_rf</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_rf</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="mi">0</span><span class="p">.</span><span class="mi">6080482109223608</span>
</pre></div>


<h1>Recursive Feature Elimination</h1>
<p>Recursive Feature Elimination is similar to the methods above which selects a important features that are deemed most important by the model. In simple term, it is a backward selection of the variables. Literally, this technique begins by building a model on the entire set of variables and computing an importance score for each variable. The least important variables are removed, the model is re-built, and importance scores are computed again. </p>
<div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>

<span class="n">select</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>

<span class="n">select</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># visualize the selected features:</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">select</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">mask</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray_r&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">matplotlib</span><span class="p">.</span><span class="n">image</span><span class="p">.</span><span class="n">AxesImage</span> <span class="k">at</span> <span class="mi">0</span><span class="n">x1bf8c2ce3c8</span><span class="o">&gt;</span>
</pre></div>


<p><img alt="png" src="output_26_1.png"></p>
<div class="highlight"><pre><span></span><span class="n">X_train_rfe</span> <span class="o">=</span> <span class="n">select</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_rfe</span> <span class="o">=</span> <span class="n">select</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_rfe</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test_rfe</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="mi">0</span><span class="p">.</span><span class="mi">6615123564812166</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="n">select</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span></span><span class="mi">0</span><span class="p">.</span><span class="mi">797557145968709</span>
</pre></div>


<p>The advantage of this approach is that it will not remove variables which were deemed insignificant at the beginning of the process, but become more and more significant as lesser features are removed. For datasets with many variables relatively strongly correlated with one another and relatively weakly correlated with the target variable, this approach may result in slightly different feature choices from those made by model-based selection. The disadvantage is that since you have to train the model many times, this approach is multiplicatively slower than the one-and-done.</p>
 </div>
</div>
 </div>
</body>
</html>
